{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "확률적 경사하강법.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2HKXrkMlt7LSMkdWJVcxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rumcrush/study/blob/master/%ED%99%95%EB%A5%A0%EC%A0%81_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실함수 구하기 \n",
        "* 손실 함수를 구하기 위해 경사하강법, 확률적 경사하강법 사용 \n",
        "* 둘은 손실함수의 기울기 계산에 사용되는 데이터셋 규모만 제외하고 같다 \n",
        "* 중요한 것은 손실함수의 경사를 구하는 대상 "
      ],
      "metadata": {
        "id": "Tbpq_KdZ7Zid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 배치 경사 하강법 \n",
        "* 경사 하강법의 손실 함수 기울기 계산에 전체 학습 데이터셋의 크기와 동일하게 잡는 방법\n",
        "* 경사 하강법 대상이 배치 크기와 동일함\n",
        "* 데이터셋 전체가 대상, 파라미터가 한번 이동할 때 마다 계산할 값이 너무 많음\n",
        "* 너무 긴 계산시간, 소모되는 메모리 양이 너무 많음. 리소스 소모가 지나치게 크다\n",
        "* 파라미터 업데이트 수가 적다. 랜덤하게 뽑힌 시작 위치의 가중치 수도 적어서 Local minimum현상이 발생 확률 높음"
      ],
      "metadata": {
        "id": "oQIG8WAQ67JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 확률적 경사하강법 \n",
        "\n",
        "* 배치 경사 하강법을 개선한 방법 \n",
        "* 학습 데이터 셋에서 랜덤으로 한 개의 데이터셋 추출, 그 샘플에 대해서만 기울기 계싼 \n",
        "* 매 반복에서 다룰 데이터 수가 매우 적고 학습 속도가 빠름 \n",
        "* 메모리 소모량이 매우 낮고 매우 큰 훈련 데이터셋도 학습 가능 \n",
        "* 무작위로 추출된 샘플에 대한 경사를 구하기 때문에 배치 경사 하강법보다 불안정함 "
      ],
      "metadata": {
        "id": "gG0AUzBZ6vXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 학습률 스케줄 \n",
        "* 전역 최솟값에 도달하기 어렵다는 문제를 해결하기 위한 방법 \n",
        "* 학습률을 천천히 줄여서 최솟값에 다다르게 하는 방법이 있음 \n",
        "* 학습률이 작아질 수록 이동하는 양이 줄어들기 떄문에 전역 최솟값에 안정적으로 수렴할 수 있다 \n",
        "* 학습률이 너무 급격하게 감소하면 Local Optima문제나 Plateau현상이 발생할 가능성 높아짐 \n",
        "* 하지만 학습률을 너무 천천히 줄이면 최적해 주변을 맴돌 수 있음 "
      ],
      "metadata": {
        "id": "QSeVvfsy8K7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 미니 배치 경사 하강법 \n",
        "* 배치 크기를 줄이고 확률적 경사하강법을 사용하는 기법 \n",
        "* 예 : 학습 데이터 1000개, batch size 100일 경우 총 10개의 mini batch가 나옴. 이 mini batch 하나당 한번 씩 SGD(확률적 경사하강법) 진행. 1 epoch당 10번의 확률적 경사하강법을 진행\n",
        "* 전체 데이터셋을 대상으로 한 확률적 경사하강법 보다 파라미터 공간에서 shooting이 줄어드는데, 미니 배치의 손실값 평균에 대해 경사하강을 진행함 \n",
        "* 따라서 Local optima 현상 발생 할 수 있음 -> 학습량을 늘리면 해결 가능 \n",
        "* 배치 크기는 총 학습데이터 셋의 크기를 배치 크기로 나눴을 때 딱 떨어지는 크기로 하는 것이 좋다\n",
        "* 배치 크기로 나누기 어려운 경우, 나머지 숫자는 버리기 \n"
      ],
      "metadata": {
        "id": "0XzfMObf8l57"
      }
    }
  ]
}
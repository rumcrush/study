{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "활성화함수_sigmoid, ReLU함수_개념정리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZuTyFlQB6X0dzLugH1z2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rumcrush/study/blob/master/%ED%99%9C%EC%84%B1%ED%99%94%ED%95%A8%EC%88%98_sigmoid%2C_ReLU%ED%95%A8%EC%88%98_%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 활성화 함수\n",
        "\n",
        "> 입력된 데이터의 가중 합을 출력 신호로 변환하는 함수. 신경망의 목적, 레이어의 역할에 따라 선택적으로 적용.(활성화, 비 활성화를 결정) \n",
        "\n",
        "> 구조 : 노드에 입력된 비선형 함수에 통과 -> 다음 다음 레이어에 전달\n",
        "\n",
        "> 비선형 함수를 사용하는 이유? 딥러닝 모델의 레이어 층을 선형함수보다 깊게 가져갈 수 있기 때문. \n",
        "\n",
        "\n",
        "\n",
        "종류 : sigmoid, ReLU 등이 있음 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1.sigmoid 함수 \n",
        "1) Logisitc 함수\n",
        "\n",
        "2) x의 값에 따라 0~1사이의 값을 출력\n",
        "\n",
        "3) S자 형태를 지님\n",
        "\n",
        "4) 입력받는 x값이 커질수록 미분값이 소실될 가능성이 크다 \n",
        "\n",
        "5) 모든 실수값을 0보다 크고, 1보다 작은 미분 가능한 수로 변환하는 특징 \n",
        "\n",
        "6) 이진분류이며 주로 출력층에서 사용 됨 \n",
        "\n",
        "\n",
        "\n",
        "## 2.LeRU함수 \n",
        "1) 가장 많이 사용되는 활성화 함수 \n",
        "\n",
        "2) 장점 : sigmoid함수 보다 학습이 빠르고, 연산 비용이 적고 구현이 간단하다 \n",
        "\n",
        "3) 입력값 x가 0보다 크면 기울기가 1\n",
        "\n",
        "4) 입력값 x가 0보다 작으면 함수값이 0이 됨\n",
        "\n",
        "5) 단점 : 0보다 작은 값일 경우 뉴런이 죽을 수 있다 \n"
      ],
      "metadata": {
        "id": "CQ85OzfG2oi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. softmax 함수 \n",
        "1) 3개 이상으로 분류하는 다중 클래스 분류에서 사용되는 활성화 함수 \n",
        "\n",
        "2) 소프트맥스 하수는 분류될 클래스가 n개라고 할 때, n차원 벡터를 입력받아 각 클래스에 속할 확률을 추정 \n",
        "\n",
        "3) 다중 분류이다 \n",
        "\n",
        "**4) 확률의 총 합이 1이며 어떤 분류에 속할 확률이 가장 높을지 쉽게 인지 가능**\n",
        "\n",
        "4) 주로 출력층에서 사용 됨 "
      ],
      "metadata": {
        "id": "nab8g3Yl_3yC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 하이퍼볼릭 탄젠트 함수 \n",
        "\n",
        "1) sigmoid함수의 대체제로 사용 \n",
        "\n",
        "### vs sigmoid 함수\n",
        "\n",
        "* 중앙값이 0. 경사하강법 사용 시 sigmoid함수에서 발생하는 편향이동이 발생하지 않음. 즉 기울기가 양수, 음수 모두 나올 수 있어서 **sigmoid함수 보다 학습 효율이 높다** \n",
        "\n",
        "* sigmoid함수 보다 범위가 넓기 때문에 변화폭이 더 크고 기울기 소실 증상이 적다 \n",
        "\n",
        "(기울기 소실 : 미분함수에 대해 값이 일정 이상 커지는 경우 미분값이 소실되는 현상) \n",
        "\n",
        "* 은닉층에서 sigmoid함수와 같은 역할을 하는 레이어를 쌓으려면 하이퍼볼릭탄젠트를 사용하는 것이 효과적\n",
        "\n",
        "* 하지만 구간 자체가 크진 않기 때문에 기울기 소실 현상 문제는 여전히 존재함 \n"
      ],
      "metadata": {
        "id": "HxWqQuyGAlsi"
      }
    }
  ]
}